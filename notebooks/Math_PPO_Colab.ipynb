{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math PPO Colab Walkthrough - raju\n",
        "\n",
        "This notebook mirrors the math RLHF pipeline and runs comfortably on Google Colab. Run the cells in order to clone the repository, install dependencies, and launch PPO fine-tuning with the math reward model. **Run the NumPy pin/restart cell once before anything else.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Run once before continuing ----\n",
        "# Pins NumPy 2.1.3, then forces a runtime restart (required for dtype compatibility).\n",
        "%pip install --upgrade --force-reinstall --no-cache-dir \"numpy==2.1.3\"\n",
        "\n",
        "import os, IPython\n",
        "print(\"Restarting runtime to load NumPy 2.1.3 ...\")\n",
        "IPython.display.clear_output()\n",
        "os.kill(os.getpid(), 9)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Install commands are provided in Section 3 after cloning the repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mount Google Drive (optional)\n",
        "\n",
        "If your SFT and reward checkpoints live on Drive (as in `RHRL_PPO.ipynb`), mount it first. Skip this step if the checkpoints are accessible locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone the repository\n",
        "\n",
        "If you forked the project, replace the URL below with your fork (e.g. `https://github.com/<username>/ppo_from_scratch.git`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: download SFT policy checkpoint directly into Drive\n",
        "%%bash\n",
        "set -e\n",
        "python -m pip install -q gdown\n",
        "SFT_DRIVE_DIR=\"/content/drive/MyDrive/rl/unsloth_sft_model\"\n",
        "mkdir -p \"${SFT_DRIVE_DIR}\"\n",
        "if [ -z \"$(ls -A \"${SFT_DRIVE_DIR}\")\" ]; then\n",
        "  echo \"Downloading SFT checkpoint into ${SFT_DRIVE_DIR}\"\n",
        "  gdown --folder https://drive.google.com/drive/folders/1EmaHJQ47OQ2waG-efGZppsiRHbcpHS-H -O \"${SFT_DRIVE_DIR}\"\n",
        "else\n",
        "  echo \"SFT checkpoint already present at ${SFT_DRIVE_DIR}\"\n",
        "fi\n",
        "mkdir -p /content/models\n",
        "ln -sfn \"${SFT_DRIVE_DIR}\" /content/models/unsloth_sft_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nagaraju-chitluru/ppo_from_scratch.git\n",
        "%cd ppo_from_scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install math extras\n",
        "\n",
        "Install the math PPO dependencies bundled with the repository. Restart the runtime if Colab prompts you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install repository in editable mode without dependencies, then manually install math extras\n",
        "%pip install -q --force-reinstall numpy==2.1.3\n",
        "%pip install -q --no-deps -e .\n",
        "%pip install -q \\\n",
        "    \"transformers==4.44.2\" \\\n",
        "    \"trl==0.9.6\" \\\n",
        "    \"accelerate==0.33.0\" \\\n",
        "    \"datasets==2.19.1\" \\\n",
        "    sentencepiece \\\n",
        "    \"sympy==1.12\" \\\n",
        "    \"peft==0.14.0\" \\\n",
        "    gdown \\\n",
        "    bitsandbytes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Verify NumPy version and restart runtime once\n",
        "\n",
        "Run the next cell to confirm that NumPy ≥ 2.0 is active. If it prints the restart reminder, go to `Runtime → Restart runtime`, then rerun the install cell above **once** before continuing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from packaging.version import Version\n",
        "\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "if Version(np.__version__) < Version(\"2.0.0\"):\n",
        "    print(\"⚠️ Detected NumPy < 2.0. Please restart the runtime (Runtime → Restart runtime) and rerun the install cell above.\")\n",
        "else:\n",
        "    print(\"✅ NumPy 2.x is active. You can proceed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure checkpoint paths\n",
        "\n",
        "Update the YAML with your SFT policy and reward model directories. By default it points to the shared Drive paths used in `RHRL_PPO.ipynb`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "config_path = Path(\"configs/math_default.yaml\")\n",
        "print(config_path.read_text())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train reward model (full dataset)\n",
        "\n",
        "Run this after updating `configs/reward_default.yaml`. With `sample_limit: null`, it will consume the entire `kira/math-dpo` split and write the LoRA adapter to `reward.training.output_dir`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python trainer/reward_train.py --config configs/reward_default.yaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run math PPO training\n",
        "\n",
        "Create a Drive symlink (optional, keeps old paths working) and execute the PPO loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/final_project/cs5446_project\n",
        "!ln -sf /content/models/unsloth_sft_model /content/drive/MyDrive/final_project/cs5446_project/unsloth_sft_model\n",
        "!python trainer/math_train.py --config configs/math_default.yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "sft_dir = Path(\"/content/drive/MyDrive/rl/unsloth_sft_model\")\n",
        "sft_model = AutoModelForCausalLM.from_pretrained(str(sft_dir))\n",
        "sft_tokenizer = AutoTokenizer.from_pretrained(str(sft_dir), use_fast=False)\n",
        "\n",
        "sft_tokenizer.padding_side = \"left\"\n",
        "\n",
        "def generate_sft(prompts, max_new_tokens=64):\n",
        "    inputs = sft_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(sft_model.device)\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = sft_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=sft_tokenizer.eos_token_id,\n",
        "    )\n",
        "    for prompt, output in zip(prompts, outputs):\n",
        "        completion_ids = output[input_length:]\n",
        "        completion = sft_tokenizer.decode(completion_ids, skip_special_tokens=True).strip()\n",
        "        print(\"Prompt:\", prompt)\n",
        "        print(\"Response (SFT):\", completion or \"<empty>\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "prompts = [\"Solve x^2 - 5x + 6 = 0. Provide the roots explicitly.\"]\n",
        "generate_sft(prompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "policy_dir = Path(\"/content/drive/MyDrive/rl/ppo_policy\")\n",
        "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(str(policy_dir))\n",
        "tokenizer = AutoTokenizer.from_pretrained(str(policy_dir))\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "def generate(prompts, max_new_tokens=64):\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(policy_model.pretrained_model.device)\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = policy_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    for prompt, output in zip(prompts, outputs):\n",
        "        completion_ids = output[input_length:]\n",
        "        completion = tokenizer.decode(completion_ids, skip_special_tokens=True).strip()\n",
        "        print(\"Prompt:\", prompt)\n",
        "        print(\"Response (PPO):\", completion or \"<empty>\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "prompts = [\"Solve x^2 - 5x + 6 = 0. Provide the roots explicitly.\"]\n",
        "generate(prompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluate on math DPO preference data\n",
        "\n",
        "(Optional) Compare SFT and PPO policies on a held-out slice of `kira/math-dpo`. Adjust `EVAL_SPLIT` and `SAMPLE_LIMIT` as needed before running the cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "EVAL_SPLIT = \"test\"            # change to \"train\" if the dataset lacks a test split\n",
        "SAMPLE_LIMIT = 200              # set to None to evaluate the entire split\n",
        "MAX_NEW_TOKENS = 256\n",
        "\n",
        "ppo_model = policy_model\n",
        "ppo_tokenizer = tokenizer\n",
        "\n",
        "\n",
        "def extract_boxed_answers(text: str) -> set[str]:\n",
        "    answers = []\n",
        "    start = text.find(\"\\\\boxed{\")\n",
        "    while start != -1:\n",
        "        i = start + len(\"\\\\boxed{\")\n",
        "        depth = 1\n",
        "        while i < len(text) and depth > 0:\n",
        "            if text[i] == \"{\":\n",
        "                depth += 1\n",
        "            elif text[i] == \"}\":\n",
        "                depth -= 1\n",
        "            i += 1\n",
        "        if depth == 0:\n",
        "            answers.append(text[start + len(\"\\\\boxed{\") : i - 1])\n",
        "        start = text.find(\"\\\\boxed{\", i)\n",
        "    return {a.strip() for a in answers if a.strip()}\n",
        "\n",
        "\n",
        "def normalize(answers: set[str]) -> set[str]:\n",
        "    return {re.sub(r\"\\s+\", \"\", a.lower()) for a in answers} if answers else set()\n",
        "\n",
        "\n",
        "def generate_completion(model, tokenizer, prompt: str, max_new_tokens: int, is_value_model: bool = False) -> str:\n",
        "    base_model = model.pretrained_model if is_value_model else model\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(base_model.device)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "    with torch.inference_mode():\n",
        "        outputs = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "        )\n",
        "    completion_ids = outputs[0][input_len:]\n",
        "    return tokenizer.decode(completion_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "def evaluate_model(model, tokenizer, label: str, is_value_model: bool = False):\n",
        "    dataset = load_dataset(\"kira/math-dpo\", split=EVAL_SPLIT)\n",
        "    if SAMPLE_LIMIT:\n",
        "        dataset = dataset.select(range(min(len(dataset), SAMPLE_LIMIT)))\n",
        "\n",
        "    rows = []\n",
        "    correct = 0\n",
        "    missing = 0\n",
        "\n",
        "    for example in dataset:\n",
        "        prompt = example[\"prompt\"].strip()\n",
        "        reference = example[\"chosen\"].strip()\n",
        "\n",
        "        completion = generate_completion(model, tokenizer, prompt, MAX_NEW_TOKENS, is_value_model=is_value_model)\n",
        "        pred = normalize(extract_boxed_answers(completion))\n",
        "        expected = normalize(extract_boxed_answers(reference))\n",
        "\n",
        "        if not pred:\n",
        "            missing += 1\n",
        "\n",
        "        rows.append(\n",
        "            {\n",
        "                \"prompt\": prompt,\n",
        "                \"reference\": reference,\n",
        "                \"completion\": completion,\n",
        "                \"pred_boxed\": sorted(pred),\n",
        "                \"ref_boxed\": sorted(expected),\n",
        "                \"is_correct\": bool(pred and pred == expected),\n",
        "            }\n",
        "        )\n",
        "        if pred and pred == expected:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / len(rows) if rows else 0.0\n",
        "    print(f\"{label} accuracy: {accuracy:.3f} ({correct}/{len(rows)}) — missing boxed answers: {missing}\")\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "print(\"Evaluating SFT policy...\")\n",
        "sft_results = evaluate_model(sft_model, sft_tokenizer, label=\"SFT\")\n",
        "print(\"Evaluating PPO policy...\")\n",
        "ppo_results = evaluate_model(ppo_model, ppo_tokenizer, label=\"PPO\", is_value_model=True)\n",
        "\n",
        "sft_results.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inspect artifacts\n",
        "\n",
        "Training outputs (policy checkpoints, reward traces, evaluation summaries) are written to the directory specified in `training.save_dir`. Adjust batch sizes, `target_kl`, and reward weights in `math_default.yaml` to run longer experiments after verifying the pipeline end to end.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
