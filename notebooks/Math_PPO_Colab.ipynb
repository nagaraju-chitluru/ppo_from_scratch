{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math PPO Colab Walkthrough\n",
        "\n",
        "This notebook mirrors the math RLHF pipeline and runs comfortably on Google Colab. Run the cells in order to clone the repository, install dependencies, and launch PPO fine-tuning with the math reward model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Install commands are provided in Section 3 after cloning the repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mount Google Drive (optional)\n",
        "\n",
        "If your SFT and reward checkpoints live on Drive (as in `RHRL_PPO.ipynb`), mount it first. Skip this step if the checkpoints are accessible locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone the repository\n",
        "\n",
        "If you forked the project, replace the URL below with your fork (e.g. `https://github.com/<username>/ppo_from_scratch.git`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: download SFT policy checkpoint directly into Drive\n",
        "%%bash\n",
        "set -e\n",
        "python -m pip install -q gdown\n",
        "SFT_DRIVE_DIR=\"/content/drive/MyDrive/rl/unsloth_sft_model\"\n",
        "mkdir -p \"${SFT_DRIVE_DIR}\"\n",
        "if [ -z \"$(ls -A \"${SFT_DRIVE_DIR}\")\" ]; then\n",
        "  echo \"Downloading SFT checkpoint into ${SFT_DRIVE_DIR}\"\n",
        "  gdown --folder https://drive.google.com/drive/folders/1EmaHJQ47OQ2waG-efGZppsiRHbcpHS-H -O \"${SFT_DRIVE_DIR}\"\n",
        "else\n",
        "  echo \"SFT checkpoint already present at ${SFT_DRIVE_DIR}\"\n",
        "fi\n",
        "mkdir -p /content/models\n",
        "ln -sfn \"${SFT_DRIVE_DIR}\" /content/models/unsloth_sft_model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nagaraju-chitluru/ppo_from_scratch.git\n",
        "%cd ppo_from_scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install math extras\n",
        "\n",
        "Install the math PPO dependencies bundled with the repository. Restart the runtime if Colab prompts you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install repository in editable mode without dependencies, then manually install math extras\n",
        "%pip install -q --force-reinstall numpy==2.1.3\n",
        "%pip install -q --no-deps -e .\n",
        "%pip install -q \\\n",
        "    \"transformers==4.44.2\" \\\n",
        "    \"trl==0.9.6\" \\\n",
        "    \"accelerate==0.33.0\" \\\n",
        "    \"datasets==2.19.1\" \\\n",
        "    sentencepiece \\\n",
        "    \"sympy==1.12\" \\\n",
        "    \"peft==0.14.0\" \\\n",
        "    gdown \\\n",
        "    bitsandbytes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Verify NumPy version and restart runtime once\n",
        "\n",
        "Run the next cell to confirm that NumPy ≥ 2.0 is active. If it prints the restart reminder, go to `Runtime → Restart runtime`, then rerun the install cell above **once** before continuing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from packaging.version import Version\n",
        "\n",
        "print(\"NumPy version:\", np.__version__)\n",
        "if Version(np.__version__) < Version(\"2.0.0\"):\n",
        "    print(\"⚠️ Detected NumPy < 2.0. Please restart the runtime (Runtime → Restart runtime) and rerun the install cell above.\")\n",
        "else:\n",
        "    print(\"✅ NumPy 2.x is active. You can proceed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure checkpoint paths\n",
        "\n",
        "Update the YAML with your SFT policy and reward model directories. By default it points to the shared Drive paths used in `RHRL_PPO.ipynb`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "config_path = Path(\"configs/math_default.yaml\")\n",
        "print(config_path.read_text())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run math PPO training\n",
        "\n",
        "Create a Drive symlink (optional, keeps old paths working) and execute the PPO loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/final_project/cs5446_project\n",
        "!ln -sf /content/models/unsloth_sft_model /content/drive/MyDrive/final_project/cs5446_project/unsloth_sft_model\n",
        "!python trainer/math_train.py --config configs/math_default.yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "sft_dir = Path(\"/content/drive/MyDrive/rl/unsloth_sft_model\")\n",
        "sft_model = AutoModelForCausalLM.from_pretrained(str(sft_dir))\n",
        "sft_tokenizer = AutoTokenizer.from_pretrained(str(sft_dir), use_fast=False)\n",
        "\n",
        "sft_tokenizer.padding_side = \"left\"\n",
        "\n",
        "def generate_sft(prompts, max_new_tokens=64):\n",
        "    inputs = sft_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(sft_model.device)\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = sft_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=sft_tokenizer.eos_token_id,\n",
        "    )\n",
        "    for prompt, output in zip(prompts, outputs):\n",
        "        completion_ids = output[input_length:]\n",
        "        completion = sft_tokenizer.decode(completion_ids, skip_special_tokens=True).strip()\n",
        "        print(\"Prompt:\", prompt)\n",
        "        print(\"Response (SFT):\", completion or \"<empty>\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "prompts = [\"Solve x^2 - 5x + 6 = 0. Provide the roots explicitly.\"]\n",
        "generate_sft(prompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from trl import AutoModelForCausalLMWithValueHead\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "policy_dir = Path(\"/content/drive/MyDrive/rl/ppo_policy\")\n",
        "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(str(policy_dir))\n",
        "tokenizer = AutoTokenizer.from_pretrained(str(policy_dir))\n",
        "tokenizer.padding_side = \"left\"\n",
        "\n",
        "def generate(prompts, max_new_tokens=64):\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(policy_model.pretrained_model.device)\n",
        "    input_length = inputs[\"input_ids\"].shape[1]\n",
        "    outputs = policy_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    for prompt, output in zip(prompts, outputs):\n",
        "        completion_ids = output[input_length:]\n",
        "        completion = tokenizer.decode(completion_ids, skip_special_tokens=True).strip()\n",
        "        print(\"Prompt:\", prompt)\n",
        "        print(\"Response (PPO):\", completion or \"<empty>\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "prompts = [\"Solve x^2 - 5x + 6 = 0. Provide the roots explicitly.\"]\n",
        "generate(prompts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inspect artifacts\n",
        "\n",
        "Training outputs (policy checkpoints, reward traces, evaluation summaries) are written to the directory specified in `training.save_dir`. Adjust batch sizes, `target_kl`, and reward weights in `math_default.yaml` to run longer experiments after verifying the pipeline end to end.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
