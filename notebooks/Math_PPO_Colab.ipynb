{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Math PPO Colab Walkthrough\n",
        "\n",
        "This notebook mirrors the math RLHF pipeline and runs comfortably on Google Colab. Run the cells in order to clone the repository, install dependencies, and launch PPO fine-tuning with the math reward model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mount Google Drive (optional)\n",
        "\n",
        "If your SFT and reward checkpoints live on Drive (as in `RHRL_PPO.ipynb`), mount it first. Skip this step if the checkpoints are accessible locally.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone the repository\n",
        "\n",
        "If you forked the project, replace the URL below with your fork (e.g. `https://github.com/<username>/ppo_from_scratch.git`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/nagaraju-chitluru/ppo_from_scratch.git\n",
        "%cd ppo_from_scratch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install math extras\n",
        "\n",
        "Install the math PPO dependencies bundled with the repository. Restart the runtime if Colab prompts you.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install repository in editable mode without dependencies, then manually install math extras\n",
        "%pip install -q --no-deps -e .\n",
        "%pip install -q \\\n",
        "    \"numpy==1.26.4\" \\\n",
        "    \"transformers==4.44.2\" \\\n",
        "    \"trl==0.9.6\" \\\n",
        "    \"accelerate==0.33.0\" \\\n",
        "    \"datasets==2.19.1\" \\\n",
        "    \"sentencepiece\" \\\n",
        "    \"sympy==1.12\" \\\n",
        "    \"peft==0.14.0\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Configure checkpoint paths\n",
        "\n",
        "Update the YAML with your SFT policy and reward model directories. By default it points to the shared Drive paths used in `RHRL_PPO.ipynb`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "config_path = Path(\"configs/math_default.yaml\")\n",
        "print(config_path.read_text())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run math PPO training\n",
        "\n",
        "This executes the TRL-based PPO loop that samples math prompts, scores responses with the reward model, and updates the policy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python trainer/math_train.py --config configs/math_default.yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Inspect artifacts\n",
        "\n",
        "Training outputs (policy checkpoints, reward traces, evaluation summaries) are written to the directory specified in `training.save_dir`. Adjust batch sizes, `target_kl`, and reward weights in `math_default.yaml` to run longer experiments after verifying the pipeline end to end.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
